import json, os
import re
from typing import List, Dict, Any, Callable, Optional
from storyteller.llm_call.openai_llm import call_openai  # å‡è®¾ä½ æœ‰ç±»ä¼¼çš„è°ƒç”¨å°è£…
from storyteller.llm_call.prompt_factory import get_prompt

def generate_diverse_responses(
    prompt_template: str, 
    prompt_params: Dict[str, Any],
    llm_kwargs: Dict[str, Any], 
    n: int = 6,
    response_processor: Optional[Callable] = None
) -> List[Any]:
    """
    é€šç”¨å¤šæ ·æ€§å“åº”ç”Ÿæˆå‡½æ•°ï¼Œå¯ç”¨äºä¸åŒç±»å‹çš„å“åº”ç”Ÿæˆ
    
    å‚æ•°:
        prompt_template: æç¤ºè¯æ¨¡æ¿åç§°
        prompt_params: æç¤ºè¯å‚æ•°
        llm_kwargs: LLMè°ƒç”¨å‚æ•°
        n: ç”Ÿæˆçš„å“åº”æ•°é‡
        response_processor: è‡ªå®šä¹‰çš„å“åº”å¤„ç†å‡½æ•°
    
    è¿”å›:
        å¤„ç†åçš„å“åº”åˆ—è¡¨
    """
    # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæç¤ºè¯
    prompt = get_prompt(prompt_template, prompt_params)
    
    # åˆ›å»ºä¸´æ—¶å‚æ•°ï¼Œå»æ‰nå‚æ•°
    llm_kwargs_temp = llm_kwargs.copy()
    if 'n' in llm_kwargs_temp:
        del llm_kwargs_temp['n']
    
    # å­˜å‚¨æ‰€æœ‰å“åº”
    all_raw_responses = []
    
    # ä½¿ç”¨configä¸­çš„temperatureï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼0.7
    base_temperature = llm_kwargs_temp.get('temperature', 0.7)
    
    # å¤šæ¬¡ç‹¬ç«‹è°ƒç”¨APIä»¥è·å–å¤šä¸ªå“åº”
    print(f"ğŸ”„ é€šè¿‡ {n} æ¬¡ç‹¬ç«‹APIè°ƒç”¨è·å–å¤šæ ·åŒ–å“åº”")
    
    for i in range(n):
        # ä¸ºæ¯æ¬¡è°ƒç”¨æ·»åŠ å¾®å°æ‰°åŠ¨ä»¥å¢åŠ å¤šæ ·æ€§
        temp_kwargs = llm_kwargs_temp.copy()
        # åœ¨åŸºç¡€æ¸©åº¦ä¸Šå¢åŠ å°‘é‡éšæœºæ‰°åŠ¨
        temp_variation = (i % 3 - 1) * 0.05  # -0.05, 0, +0.05 å¾ªç¯å˜åŒ–
        temp_kwargs['temperature'] = max(0.1, min(1.0, base_temperature + temp_variation))
        
        print(f"  è°ƒç”¨ {i+1}/{n} (temperature={temp_kwargs['temperature']:.2f})")
        
        try:
            # å•æ¬¡è°ƒç”¨APIè·å–ä¸€ä¸ªå“åº”
            response = call_openai(prompt, **temp_kwargs)
            if response and len(response) > 0:
                all_raw_responses.append(response[0])
                print(f"  âœ“ è·å–åˆ°å“åº” (é•¿åº¦: {len(response[0])})")
            else:
                print(f"  âœ— æœªè·å–åˆ°æœ‰æ•ˆå“åº”")
        except Exception as e:
            print(f"  âœ— APIè°ƒç”¨å¤±è´¥: {str(e)}")
            continue
    
    # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
    print(f"è·å–åˆ° {len(all_raw_responses)} ä¸ªåŸå§‹å“åº”")
    
    # å¤„ç†å“åº”
    if response_processor:
        # ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨
        processed_responses = []
        for i, r in enumerate(all_raw_responses):
            try:
                processed = response_processor(r)
                if processed:
                    processed_responses.append(processed)
                    print(f"å“åº” {i+1}: æˆåŠŸå¤„ç†")
                else:
                    print(f"å“åº” {i+1}: å¤„ç†ç»“æœä¸ºç©ºï¼Œå·²è·³è¿‡")
            except Exception as e:
                print(f"å“åº” {i+1}: å¤„ç†å¼‚å¸¸ - {str(e)}")
                continue
        return processed_responses
    else:
        # é»˜è®¤å¤„ç†ï¼šæ¸…ç†JSONå¹¶è§£æ
        processed_responses = []
        for i, r in enumerate(all_raw_responses):
            try:
                cleaned = clean_json_response(r)
                parsed = json.loads(cleaned)
                processed_responses.append(parsed)
                print(f"å“åº” {i+1}: æˆåŠŸè§£æJSON")
            except Exception as e:
                print(f"å“åº” {i+1}: å¤„ç†å¼‚å¸¸ - {str(e)}")
                continue
        return processed_responses


def clean_json_response(response: str) -> str:
    """
    æ¸…ç† LLM è¿”å›çš„ JSON å“åº”ï¼Œç§»é™¤ Markdown ä»£ç å—æ ‡è®°ã€‚
    """
    response = re.sub(r'^```(?:json)?\s*', '', response)
    response = re.sub(r'\s*```$', '', response)
    return response.strip()


def build_clustering_prompt(
    responses: List[Any],
    clustering_config: Dict[str, Any],
    context_info: Dict[str, Any]
) -> str:
    """
    æ„å»ºèšç±»æç¤ºè¯ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„å“åº”èšç±»
    
    å‚æ•°:
        responses: å“åº”åˆ—è¡¨
        clustering_config: èšç±»é…ç½®(åŒ…å«å“åº”æ ¼å¼åŒ–å‡½æ•°ã€èšç±»æ ‡å‡†ç­‰)
        context_info: ä¸Šä¸‹æ–‡ä¿¡æ¯(æŸ¥è¯¢ã€æ•°æ®ä¸Šä¸‹æ–‡ç­‰)
    
    è¿”å›:
        èšç±»æç¤ºè¯
    """
    # è·å–æ ¼å¼åŒ–å‡½æ•°ï¼Œæˆ–ä½¿ç”¨é»˜è®¤æ ¼å¼åŒ–
    format_func = clustering_config.get("format_func", lambda x, i: f"æ–¹æ¡ˆ{i+1}: {json.dumps(x, ensure_ascii=False)}")
    
    # æ ¼å¼åŒ–å“åº”
    responses_str = "\n\n".join([
        format_func(resp, i) for i, resp in enumerate(responses)
    ])
    
    # è·å–èšç±»æ ‡å‡†
    similarity_criteria = clustering_config.get("similarity_criteria", 
        "- å¦‚æœå®ƒä»¬åˆ†æç›¸åŒçš„æ•°æ®ç»´åº¦æˆ–ç‰¹å¾\n"
        "- å¦‚æœå®ƒä»¬é‡‡ç”¨ç›¸ä¼¼çš„åˆ†æç»“æ„æˆ–æ¡†æ¶\n"
        "- å¦‚æœå®ƒä»¬å…³æ³¨ç›¸åŒçš„æ•°æ®å…³ç³»æˆ–æ¨¡å¼"
    )
    
    difference_criteria = clustering_config.get("difference_criteria",
        "- å¦‚æœå®ƒä»¬å…³æ³¨å®Œå…¨ä¸åŒçš„æ•°æ®ç»´åº¦\n"
        "- å¦‚æœå®ƒä»¬ä½¿ç”¨ä¸åŒçš„åˆ†ææ¡†æ¶æˆ–é€»è¾‘\n"
        "- å¦‚æœå®ƒä»¬è§£å†³æŸ¥è¯¢çš„ä¸åŒæ–¹é¢"
    )
    
    # æ˜ç¡®å¯ç”¨çš„ç´¢å¼•èŒƒå›´
    valid_indices = list(range(len(responses)))
    max_index = len(responses) - 1
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""
æ‚¨æ­£åœ¨è¯„ä¼°å‡ ä¸ªå€™é€‰{clustering_config.get('item_type', 'æ–¹æ¡ˆ')}ï¼Œå®ƒä»¬éƒ½æ˜¯é’ˆå¯¹åŒä¸€æ•°æ®åˆ†ææŸ¥è¯¢çš„ã€‚

=== å€™é€‰{clustering_config.get('item_type', 'æ–¹æ¡ˆ')} ===
{responses_str}

=== ä»»åŠ¡ä¿¡æ¯ ===
æŸ¥è¯¢: {context_info.get('query', '')}
{f"æ•°æ®ä¸Šä¸‹æ–‡: {context_info.get('data_context', '')}" if 'data_context' in context_info else ''}

### æŒ‡å¼•:

æ‚¨çš„ä»»åŠ¡æ˜¯å°†è¿™äº›{clustering_config.get('item_type', 'æ–¹æ¡ˆ')}åˆ†ç»„æˆä¸åŒçš„ã€äº’ä¸é‡å çš„èšç±»ï¼ŒåŸºäºå®ƒä»¬çš„åŸºæœ¬åˆ†ææ–¹æ³•ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦:

1. æ¯ä¸ªæ–¹æ¡ˆå¿…é¡»ä¸”åªèƒ½å±äºä¸€ä¸ªèšç±»ã€‚
2. ä¸¤ä¸ªæ–¹æ¡ˆåº”è¯¥åœ¨åŒä¸€èšç±»ä¸­ï¼Œå½“ä¸”ä»…å½“å®ƒä»¬ä»£è¡¨æœ¬è´¨ä¸Šç›¸åŒçš„åˆ†ææ¡†æ¶ï¼Œå³ä½¿ä½¿ç”¨ä¸åŒçš„æªè¾ã€‚
3. èšç±»åº”è¯¥ä»£è¡¨æ ¹æœ¬ä¸åŒçš„åˆ†ææ–¹æ³•æˆ–ç»“æ„ã€‚

ç›¸ä¼¼{clustering_config.get('item_type', 'æ–¹æ¡ˆ')}çš„æ ‡å‡†(åº”å½’ä¸ºåŒä¸€èšç±»):
{similarity_criteria}

ä¸åŒ{clustering_config.get('item_type', 'æ–¹æ¡ˆ')}çš„æ ‡å‡†(åº”å½’ä¸ºä¸åŒèšç±»):
{difference_criteria}

### é‡è¦çº¦æŸ:
- æ¯ä¸ªå“åº”ç´¢å¼•(0, 1, 2ç­‰)å¿…é¡»å‡ºç°åœ¨ä¸”ä»…å‡ºç°åœ¨ä¸€ä¸ªèšç±»ä¸­
- æœ‰æ•ˆçš„å“åº”ç´¢å¼•ä»…ä¸º: {valid_indices} (ä»0åˆ°{max_index})
- ä¸è¦åˆ›å»ºåœ¨response_indicesä¸Šæœ‰é‡å çš„èšç±»
- ç¡®ä¿æ‰€æœ‰å“åº”ç´¢å¼•éƒ½åŒ…å«åœ¨æ°å¥½ä¸€ä¸ªèšç±»ä¸­
- ä¸è¦ä½¿ç”¨ä»»ä½•å¤§äº{max_index}æˆ–å°äº0çš„ç´¢å¼•

### è¾“å‡º:
è¯·è¿”å›å¦‚ä¸‹JSON:
```json
{{
  "type": "multiple",
  "results": [
    {{
      "cluster_id": 1,
      "response_indices": [0, 2],  // æ¯ä¸ªå“åº”å±äºä¸”åªå±äºä¸€ä¸ªèšç±»
      "top_index": 0,              // è¯¥èšç±»ä¸­æœ€å…·ä»£è¡¨æ€§çš„å“åº”
      "content": object            // ä»£è¡¨æ€§å“åº”çš„å†…å®¹(è§†å“åº”ç±»å‹è€Œå®š)
    }},
    {{
      "cluster_id": 2, 
      "response_indices": [1, 3],  // æ³¨æ„: ä¸å…¶ä»–èšç±»çš„ç´¢å¼•æ²¡æœ‰é‡å 
      "top_index": 1,
      "content": object
    }}
  ]
}}
```

è¯·è®°ä½ï¼šæœ€é‡è¦çš„è§„åˆ™æ˜¯æ¯ä¸ªå“åº”ç´¢å¼•å¿…é¡»å‡ºç°åœ¨ä¸”ä»…å‡ºç°åœ¨ä¸€ä¸ªèšç±»ä¸­ã€‚æ²¡æœ‰å“åº”åº”è¯¥åœ¨å¤šä¸ªèšç±»ä¸­ï¼Œæ¯ä¸ªå“åº”å¿…é¡»åœ¨æŸä¸ªèšç±»ä¸­ã€‚

è¯·è®°ä½ï¼šæ‚¨åªèƒ½ä½¿ç”¨0åˆ°{max_index}ä¹‹é—´çš„ç´¢å¼•(åŒ…æ‹¬0å’Œ{max_index})ã€‚ä¸è¦ä½¿ç”¨ä»»ä½•è¶…å‡ºæ­¤èŒƒå›´çš„ç´¢å¼•ï¼

è¯·ä»…è¿”å›JSONè¾“å‡ºã€‚
    """
    return prompt


def run_universal_self_consistency(
    responses_config: Dict[str, Any],
    clustering_config: Dict[str, Any],
    context_info: Dict[str, Any],
    llm_kwargs: Dict[str, Any],
    response_processor: Optional[Callable] = None,
    content_extractor: Optional[Callable] = None
) -> List[Dict[str, Any]]:
    """
    é€šç”¨USCæµç¨‹ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„å“åº”ç”Ÿæˆå’Œèšç±»
    
    å‚æ•°:
        responses_config: å“åº”ç”Ÿæˆé…ç½®
        clustering_config: èšç±»é…ç½®
        context_info: ä¸Šä¸‹æ–‡ä¿¡æ¯
        llm_kwargs: LLMè°ƒç”¨å‚æ•°
        response_processor: å“åº”å¤„ç†å‡½æ•°
        content_extractor: ä»èšç±»ç»“æœä¸­æå–å†…å®¹çš„å‡½æ•°
    
    è¿”å›:
        èšç±»ç»“æœåˆ—è¡¨
    """
    from storyteller.llm_call.openai_llm import call_openai
    
    # ä½¿ç”¨é…ç½®ç”Ÿæˆå¤šæ ·åŒ–å“åº”
    prompt_template = responses_config.get("prompt_template")
    prompt_params = responses_config.get("prompt_params", {})
    n = responses_config.get("n", 6)
    
    # ç”Ÿæˆå“åº”
    responses = generate_diverse_responses(
        prompt_template=prompt_template,
        prompt_params=prompt_params,
        llm_kwargs=llm_kwargs,
        n=n,
        response_processor=response_processor
    )
    
    print(f"ğŸ“‹ å…±è·å–åˆ° {len(responses)} ä¸ªæœ‰æ•ˆå“åº”")
    
    # å¦‚æœå“åº”æ•°é‡ä¸å¤Ÿï¼Œæ— æ³•èšç±»
    if len(responses) < 2:
        print("âš ï¸ å“åº”æ•°é‡ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œèšç±»")
        if responses:
            # åˆ›å»ºä¸€ä¸ªå•ä¸€èšç±»
            return [{
                "cluster_id": 1,
                "response_indices": [0],
                "top_index": 0,
                "content": responses[0]
            }]
        return []
    
    # æ„å»ºèšç±»æç¤ºè¯
    usc_prompt = build_clustering_prompt(
        responses=responses,
        clustering_config=clustering_config,
        context_info=context_info
    )
    
    print(f"ğŸ” ç”Ÿæˆèšç±»åˆ†ææç¤ºè¯ï¼Œé•¿åº¦: {len(usc_prompt)} å­—ç¬¦")
    
    # è¿›è¡Œèšç±»åˆ†æ
    usc_response = call_openai(usc_prompt, **llm_kwargs)
    
    # æ¸…ç†å“åº”
    cleaned = clean_json_response(usc_response[0])
    
    try:
        # è§£æèšç±»ç»“æœ
        usc_result = json.loads(cleaned)
        clusters = usc_result.get("results", [])
        
        # ä½¿ç”¨å†…å®¹æå–å™¨æˆ–é»˜è®¤æ–¹æ³•æå–å†…å®¹
        if content_extractor:
            for cluster in clusters:
                top_index = cluster.get("top_index", 0)
                if 0 <= top_index < len(responses):
                    cluster["content"] = content_extractor(responses[top_index])
        else:
            # é»˜è®¤ï¼šç›´æ¥ä½¿ç”¨top_indexå¯¹åº”çš„å“åº”ä½œä¸ºå†…å®¹
            for cluster in clusters:
                top_index = cluster.get("top_index", 0)
                if 0 <= top_index < len(responses):
                    cluster["content"] = responses[top_index]
        
        print(f"âœ… æˆåŠŸèšç±»ä¸º {len(clusters)} ç»„")
        return clusters
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {str(e)}")
        # å¦‚æœèšç±»å¤±è´¥ï¼Œä½†æœ‰å“åº”ï¼Œè¿”å›æ¯ä¸ªå“åº”ä½œä¸ºå•ç‹¬çš„èšç±»
        fallback_clusters = []
        for i, resp in enumerate(responses):
            fallback_clusters.append({
                "cluster_id": i + 1,
                "response_indices": [i],
                "top_index": i,
                "content": resp
            })
        print(f"âš ï¸ èšç±»å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆï¼Œåˆ›å»º {len(fallback_clusters)} ä¸ªå•å“åº”èšç±»")
        return fallback_clusters


if __name__ == "__main__":
    query = "Analyze the difference between the customers"

    json_path = os.path.join("storyteller", "dataset", "data_context.json")
    with open(json_path, 'r', encoding='utf-8') as f:
                            data_context = json.load(f)
    
    # ç¡®ä¿data_contextæ˜¯å­—ç¬¦ä¸²ç±»å‹
    if isinstance(data_context, dict):
        data_context = json.dumps(data_context, ensure_ascii=False)
    
    print(f"åŠ è½½æ•°æ®ä¸Šä¸‹æ–‡: {json_path}")

    llm_kwargs = {
        "model": "gpt-4o",
        "temperature": 0.7,
        "max_tokens": 2048,
    }

    # ä»é…ç½®æ–‡ä»¶è¯»å–APIä¿¡æ¯
    config_path = os.path.join("storyteller", "config", "config.yaml")
    if os.path.exists(config_path):
        import yaml
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                if 'llm_kwargs' in config:
                    # æ›´æ–°llm_kwargsï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
                    llm_kwargs.update(config['llm_kwargs'])
                    print(f"âœ“ æˆåŠŸä»é…ç½®æ–‡ä»¶åŠ è½½APIè®¾ç½®")
        except Exception as e:
            print(f"âœ— è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")

    print(">>> æ­£åœ¨è¿è¡Œ Universal Self-Consistency æµç¨‹...")
    # ä½¿ç”¨è¾ƒå°çš„nå€¼ï¼Œé¿å…ç”Ÿæˆè¿‡å¤šæ–¹æ¡ˆå¯¼è‡´è¶…æ—¶
    clusters = run_universal_self_consistency(
        responses_config={
            "prompt_template": "Query2Chapters",
            "prompt_params": {
                "QUERY": query,
                "DATA_CONTEXT": data_context
            },
            "n": 4
        },
        clustering_config={
            "item_type": "ç« èŠ‚ç»“æ„",
            "format_func": lambda resp, i: f"æ–¹æ¡ˆ{i+1}: {resp.get('chapters', [])}",
            "similarity_criteria": "- ä½¿ç”¨ç›¸ä¼¼æˆ–é‡æ–°æªè¾ä½†è¦†ç›–ç›¸åŒåˆ†æç»´åº¦çš„ç« èŠ‚\n- ç›¸åŒåˆ†æä¸»é¢˜çš„ç»†å¾®é‡æ–°æ’åº\n- ç›¸åŒé€šç”¨ä¸»é¢˜çš„ä¸åŒç‰¹å®šæ€§çº§åˆ«",
            "difference_criteria": "- å®Œå…¨ä¸åŒçš„åˆ†æç»´åº¦\n- é€šè¿‡ä¸åŒçš„é€»è¾‘æ¡†æ¶æ„å»ºåˆ†æ\n- è§£å†³æŸ¥è¯¢çš„ä¸åŒæ–¹é¢(ä¾‹å¦‚ï¼Œä¸€ä¸ªä¸“æ³¨äºäººå£ç»Ÿè®¡ï¼Œå¦ä¸€ä¸ªä¸“æ³¨äºæ—¶é—´æ¨¡å¼)"
        },
        context_info={
            "query": query,
            "data_context": data_context
        },
        llm_kwargs=llm_kwargs
    )

    print("\n=== USC èšç±»ç»“æœï¼ˆç« èŠ‚ç»“æ„ï¼‰ ===")
    for cluster in clusters:
        cluster_id = cluster.get("cluster_id", "æœªçŸ¥")
        indices = cluster.get("response_indices", [])
        top_index = cluster.get("top_index", None)
        chapters = cluster.get("content", [])

        print(f"ğŸ“˜ Cluster {cluster_id}:")
        print(f"  æ¥æº response index: {indices}")
        print(f"  Top response: {top_index}")
        print(f"  ç« èŠ‚ç»“æ„: {chapters}\n")